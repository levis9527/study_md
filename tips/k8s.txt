k8s

k8s概念

node 
	表示集群里面的机器，k8s必定是集群，minikube是为了测试方便才有的单机，一个node表示一个集群机器的资源，k8s并不创建node，只是管理存储再node上的资源，一般node启动的时候会通过kubelet主动向master注册
	自己，创建node资源

pod  
	pod是k8s里面的最小单元，和docker的容器不同，pod可以包含多个容器来提供服务，分组编排更好的横向拓展，类似于把容器分组，内部耦合并和外部隔离
	同一pod里面可以共享存储，共享网络等，
	pod中的应用（容器）使用同样的网络命名空间端口以及虚拟卷，可以进行无障碍通信，可以把某个docker-compose文件定义的一组容器组放一个pod，方便互相访问
	在生产环境中，推荐通过其他方式创建pod，直接创建如果出现异常导致pod被删除无法重建，比如deployment，statefulset，job，cronjob

namespace
	k8s支持多个虚拟集群，底层可以跑在一个集群上面，但是同过namespace来区分集群做到资源大的隔离，比如整个公司不同部门需要隔离的k8s可以运行在一个底层
	集群上，但是通过namespace去区分隔离不同部门的资源

ReplicationController
	一种k8s资源（副本控制器），确保pod数量少于多少时自动创建pod替换上，保持pod的副本数和配置匹配，出现问题系统会删除pod重新创建pod并运行，新的pod没有原来的数据

ReplicaSet
	新的副本控制器，对比 ReplicationController 的不同是支持更多的选择器，推荐

Deployment
	Deployment部署，可以用来定义一组pod以及指定副本数量，状态等，还能操作容器版本回滚，一般使用Deployment定义一组pod（多个副本），然后使用services对外暴露服务，那样
	service就不需要自己去创建pod而是关联Deployment，动态扩容，回滚
	Deployment的回退： 使用kubectl set image或者其他方式升级pod或者运行时一直crash looping等的时候，Rollout会卡住，replicas会出现异常，pod可能处于ImagePullBackOff（拉取镜像失败）
		状态，Deployment会自动停止坏的Rollout并停止扩容新的Replica Set，为了修复这个问题，我们只能手动回退到稳定的版本，处理完异常再重新更新
	创建deployment的时候可以指定—recored参数记录每次操作详情，那么可以使用kubectl rollout history查看历史操作记录

services
	pod的生命周期是有限的，出现严重的异常会根据规则被kill删除然后新创建副本，会导致对外提供服务的话会存在严重的不稳定的情况，比如前端调用某pod的服务，某服务出现异常导致
	被kill掉后永远连不上，所以需要在pod上增加一个中间层service整合pod访问入口来对外提供服务

job
	用来处理批处理任务的api对象，不需要长期运行，把设置的任务完成了就可以退出

DaemonSet
	后台支撑服务，DaemonSet关注所有的node而不是像其他的关注pod，比如所有节点都要加上日志监控，那么可以使用DaemonSet和nodeSelector给所选中的node加上相关的服务

StatefulSet
	有状态服务集，区别于RC和RS，RC和RS一般不挂载存储或者挂载共享存储，所有pod的状态全部是一样的，pod不存储任何数据，但是对于StatefulSet中的pod，每个pod又自己独立的存储系统，如果一个pod出现故障，
	那么会从其他node启动一个同名的pod挂载原来pod上的数据存储他的状态继续提供服务而不是挂载其他共享存储

Volume
	Volume存储卷和docker类似，Volume存储卷的生命周期和作用范围是pod，pod内所有容器共享存储卷，pod出现故障被删除后存储卷消失，有不同的存储卷类型，可以使存储卷脱离后端的实际存储技术，可以让存储管理员
	通过Persistent Volume来配置

PV和PVC
	PV和PVC使k8s集群具备了存储的逻辑抽象能力，PV和NODE是资源拥有者，PVC和POD是资源使用者，POD通过PVC声明使用PV，不一定要是本机上的文件，可以来源于网络，共享文件或者虚拟文件系统

Secret
	密钥对象，封装各种密钥敏感信息，避免直接把敏感信息写进配置文件，避免重复，减少暴露机会


k8s组件

etcd 保存整个集群的状态
API Server 提供资源操作唯一入口，提供认证授权api注册发现等服务
Controller Manager 负责维护集群状态，故障检测，自动扩展，滚动更新等
Scheduler 负责资源调度，按照预定的策略将pod调度到相应的机器上
Kubelet 负责维护容器的生命周期，也负责volume和网络的管理
Container Runtime 负责镜像管理以及pod和容器的真正运行
Kube-proxy 负责未service提供内部服务发现和负载均衡load balancer
一般来讲，deploy-->replicaSet-->pod，service通过Label Selector与后端的pod实现关联，但是不参与pod的管理，管理由deploy和rs等完成

常用操作

kubectl  port-forward service/nginx-app --address 0.0.0.0 10008:80 转发请求，本机的10008连接到service的80端口
kubectl proxy --address='0.0.0.0'  --accept-hosts='^*$'
kubectl rollout undo deployment/nginx-app 回退到上一个版本


k8s网络

默认k8spod质检是互通的，可以采用限制手段来限制外部或者不同namespace质检访问彼此，比如不让开发环境namespace为dev的pod访问到生产环境pod，不同namespace质检通信可以用服务名加上namespace访问，比如nginx.dev，
使用插件或者自己管理网络，需要保证接口的安全，尽量缩小pod的访问范围，同pod之间不会被影响，同pod质检只需要使用localhost就行，共享生命周期



使用kubeadm搭建k8s服务简易步骤

1. 准备机器（最少2台），网卡不能重复，处理器大于2个，机器已经安装好docker（必须安装）
2. 机器关闭防火墙（生产环境应该打开端口而不是关闭防火墙），禁止swap，关闭slinux
	systemctl stop firewalld && systemctl disable firewalld
	sudo swapoff -a
	setenforce 0
	sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

3. 下载k8s相关组件并设置开机重启
	cat <<EOF > /etc/yum.repos.d/kubernetes.repo
		[kubernetes]
		name=Kubernetes
		baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
		enabled=1
		gpgcheck=1
		repo_gpgcheck=1
		gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
		EOF
	yum install -y kubelet kubeadm kubectl
	systemctl enable kubelet && systemctl start kubelet

4. docker镜像拉取（因为墙的原因，kubeadm可能无法拉取镜像）
	kubeadm config images list 查看需要的镜像列表
	可以编写脚本也可以手动去dockerhub搜索相关的镜像拉取然后打标签，参考脚本如下
	set -o errexit
	set -o nounset
	set -o pipefail
	##这里定义版本
	KUBE_VERSION=v1.18.5
	KUBE_PAUSE_VERSION=3.2
	ETCD_VERSION=3.4.3-0
	DNS_VERSION=1.6.7

	GCR_URL=k8s.gcr.io
	##这里就是写你要使用的仓库
	DOCKERHUB_URL=gotok8s
	##这里是镜像列表
	images=(
	kube-proxy:${KUBE_VERSION}
	kube-scheduler:${KUBE_VERSION}
	kube-controller-manager:${KUBE_VERSION}
	kube-apiserver:${KUBE_VERSION}
	pause:${KUBE_PAUSE_VERSION}
	etcd:${ETCD_VERSION}
	coredns:${DNS_VERSION}
	)
	##这里是拉取和改名的循环语句
	for imageName in ${images[@]} ; do
	  docker pull $DOCKERHUB_URL/$imageName
	  docker tag $DOCKERHUB_URL/$imageName $GCR_URL/$imageName
	  docker rmi $DOCKERHUB_URL/$imageName
	done

5. master节点执行初始化（部分参数需要灵活设置，比如apiserver-advertise-address指定apiserver地址）
	kubeadm init --apiserver-advertise-address 192.168.56.100
	export KUBECONFIG=/etc/kubernetes/admin.conf
	kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-

6. master配置网络
	CNI bridge
	mkdir -p /etc/cni/net.d
		cat >/etc/cni/net.d/10-mynet.conf <<-EOF
		{
		    "cniVersion": "0.3.0",
		    "name": "mynet",
		    "type": "bridge",
		    "bridge": "cni0",
		    "isGateway": true,
		    "ipMasq": true,
		    "ipam": {
		        "type": "host-local",
		        "subnet": "10.244.0.0/16",
		        "routes": [
		            {"dst": "0.0.0.0/0"}
		        ]
		    }
		}
		EOF
		cat >/etc/cni/net.d/99-loopback.conf <<-EOF
		{
		    "cniVersion": "0.3.0",
		    "type": "loopback"
		}
		EOF
	flannel
		kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
	weave
		sysctl net.bridge.bridge-nf-call-iptables=1
		kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
	calico
		kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
		kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

7. 节点加入master（节点也需要完成之前的配置工作）
	master init初始化后，会生成一个命令类似kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>
	输入该命令就能加入master节点
	如果生成的token过期，那么可以使用以下的命令重新生成token
	kubeadm token create  生成token
	openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 生成证书的sha256hash值

8. 完成安装可以在master使用kubectl get node查看节点状态，如果节点存在而且状态为ready表示成功，如果有异常可以使用
	 kubectl describe node k8s-node 查看node状态，不成功的具体原因

9. 删除重新安装集群
	# drain and delete the node first
	kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
	kubectl delete node <node name>

	# then reset kubeadm
	kubeadm reset




